{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opposing Signals Analysis for Delayed Generalization\n",
    "\n",
    "This notebook provides a unified framework for analyzing opposing gradient signals during training, inspired by Rosenfeld & Risteski (2023). We track examples that have gradients pointing in opposite directions to understand their impact on delayed generalization phenomena.\n",
    "\n",
    "## Key Objectives:\n",
    "1. **Detect opposing signal examples** - Find training examples with gradients opposing the majority\n",
    "2. **Track loss dynamics** - Monitor examples with significant loss changes over time\n",
    "3. **Visualize gradient patterns** - Create interactive plots showing training dynamics\n",
    "4. **Analyze delayed generalization** - Connect opposing signals to generalization patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactive plotting\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"Plotly not available - using matplotlib only\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Tracking Infrastructure\n",
    "\n",
    "We implement a comprehensive gradient tracking system that monitors:\n",
    "- Individual example gradients\n",
    "- Gradient directions and magnitudes\n",
    "- Loss trajectories for each example\n",
    "- Opposing signal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientTracker:\n",
    "    \"\"\"Enhanced gradient tracker for opposing signals analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, track_individual_examples: bool = True):\n",
    "        self.model = model\n",
    "        self.track_individual_examples = track_individual_examples\n",
    "        \n",
    "        # Storage for analysis\n",
    "        self.gradient_history = defaultdict(list)  # epoch -> [gradients]\n",
    "        self.loss_history = defaultdict(list)      # epoch -> [losses]\n",
    "        self.example_gradients = defaultdict(lambda: defaultdict(list))  # example_id -> epoch -> gradient\n",
    "        self.example_losses = defaultdict(lambda: defaultdict(float))    # example_id -> epoch -> loss\n",
    "        \n",
    "        # Opposing signal tracking\n",
    "        self.opposing_examples = defaultdict(set)  # epoch -> {example_ids}\n",
    "        self.gradient_directions = defaultdict(list)  # epoch -> [direction_vectors]\n",
    "        \n",
    "        # Configuration\n",
    "        self.opposing_threshold = 0.1  # Cosine similarity threshold\n",
    "        self.loss_change_threshold = 0.5  # Significant loss change threshold\n",
    "        \n",
    "        print(f\"Initialized GradientTracker for model with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    def track_batch(self, epoch: int, batch_idx: int, inputs: torch.Tensor, \n",
    "                   targets: torch.Tensor, individual_losses: torch.Tensor,\n",
    "                   example_ids: Optional[List[int]] = None):\n",
    "        \"\"\"Track gradients for a single batch\"\"\"\n",
    "        \n",
    "        if example_ids is None:\n",
    "            example_ids = list(range(batch_idx * inputs.size(0), (batch_idx + 1) * inputs.size(0)))\n",
    "        \n",
    "        # Compute per-example gradients\n",
    "        batch_gradients = []\n",
    "        \n",
    "        for i, (example_id, loss) in enumerate(zip(example_ids, individual_losses)):\n",
    "            # Zero gradients\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            # Compute gradient for this example\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            # Collect gradients\n",
    "            example_grad = []\n",
    "            for param in self.model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    example_grad.append(param.grad.clone().flatten())\n",
    "            \n",
    "            if example_grad:\n",
    "                grad_vector = torch.cat(example_grad)\n",
    "                batch_gradients.append(grad_vector)\n",
    "                \n",
    "                # Store individual example data\n",
    "                if self.track_individual_examples:\n",
    "                    self.example_gradients[example_id][epoch] = grad_vector.cpu().numpy()\n",
    "                    self.example_losses[example_id][epoch] = loss.item()\n",
    "        \n",
    "        # Store batch-level data\n",
    "        if batch_gradients:\n",
    "            self.gradient_history[epoch].extend([g.cpu().numpy() for g in batch_gradients])\n",
    "            self.loss_history[epoch].extend(individual_losses.cpu().numpy().tolist())\n",
    "    \n",
    "    def detect_opposing_signals(self, epoch: int, similarity_threshold: float = -0.1) -> List[int]:\n",
    "        \"\"\"Detect examples with opposing gradient signals\"\"\"\n",
    "        \n",
    "        if epoch not in self.gradient_history:\n",
    "            return []\n",
    "        \n",
    "        gradients = np.array(self.gradient_history[epoch])\n",
    "        if len(gradients) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Compute mean gradient direction\n",
    "        mean_gradient = np.mean(gradients, axis=0)\n",
    "        mean_gradient = mean_gradient / (np.linalg.norm(mean_gradient) + 1e-8)\n",
    "        \n",
    "        # Find opposing examples\n",
    "        opposing_indices = []\n",
    "        \n",
    "        for i, grad in enumerate(gradients):\n",
    "            grad_norm = grad / (np.linalg.norm(grad) + 1e-8)\n",
    "            similarity = np.dot(mean_gradient, grad_norm)\n",
    "            \n",
    "            if similarity < similarity_threshold:  # Opposing direction\n",
    "                opposing_indices.append(i)\n",
    "        \n",
    "        self.opposing_examples[epoch] = set(opposing_indices)\n",
    "        return opposing_indices\n",
    "    \n",
    "    def analyze_loss_dynamics(self, window_size: int = 10) -> Dict[str, List[int]]:\n",
    "        \"\"\"Analyze examples with significant loss changes\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            'increasing_loss': [],\n",
    "            'decreasing_loss': [],\n",
    "            'volatile_loss': []\n",
    "        }\n",
    "        \n",
    "        for example_id, loss_dict in self.example_losses.items():\n",
    "            epochs = sorted(loss_dict.keys())\n",
    "            if len(epochs) < window_size:\n",
    "                continue\n",
    "            \n",
    "            losses = [loss_dict[epoch] for epoch in epochs]\n",
    "            \n",
    "            # Compute trends\n",
    "            recent_avg = np.mean(losses[-window_size:])\n",
    "            early_avg = np.mean(losses[:window_size])\n",
    "            volatility = np.std(losses)\n",
    "            \n",
    "            change_ratio = (recent_avg - early_avg) / (early_avg + 1e-8)\n",
    "            \n",
    "            if change_ratio > self.loss_change_threshold:\n",
    "                results['increasing_loss'].append(example_id)\n",
    "            elif change_ratio < -self.loss_change_threshold:\n",
    "                results['decreasing_loss'].append(example_id)\n",
    "            \n",
    "            if volatility > 2 * np.mean(losses):\n",
    "                results['volatile_loss'].append(example_id)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive statistics about gradient patterns\"\"\"\n",
    "        \n",
    "        stats = {\n",
    "            'total_epochs_tracked': len(self.gradient_history),\n",
    "            'total_examples_tracked': len(self.example_losses),\n",
    "            'opposing_signals_by_epoch': {},\n",
    "            'gradient_similarity_stats': {},\n",
    "            'loss_dynamics': self.analyze_loss_dynamics()\n",
    "        }\n",
    "        \n",
    "        # Opposing signals statistics\n",
    "        for epoch in self.opposing_examples:\n",
    "            stats['opposing_signals_by_epoch'][epoch] = len(self.opposing_examples[epoch])\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization Utilities\n",
    "\n",
    "Create comprehensive visualizations for understanding gradient dynamics and opposing signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpposingSignalsVisualizer:\n",
    "    \"\"\"Visualization tools for opposing signals analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, tracker: GradientTracker):\n",
    "        self.tracker = tracker\n",
    "    \n",
    "    def plot_opposing_signals_timeline(self, figsize=(15, 8)):\n",
    "        \"\"\"Plot timeline of opposing signals detection\"\"\"\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        epochs = sorted(self.tracker.opposing_examples.keys())\n",
    "        opposing_counts = [len(self.tracker.opposing_examples[epoch]) for epoch in epochs]\n",
    "        \n",
    "        # Opposing signals count over time\n",
    "        ax1.plot(epochs, opposing_counts, 'ro-', alpha=0.7, linewidth=2)\n",
    "        ax1.set_title('Opposing Signals Over Time', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Number of Opposing Examples')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss distribution by epoch\n",
    "        if epochs:\n",
    "            recent_epoch = epochs[-1]\n",
    "            recent_losses = self.tracker.loss_history[recent_epoch]\n",
    "            \n",
    "            ax2.hist(recent_losses, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax2.set_title(f'Loss Distribution (Epoch {recent_epoch})', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('Loss Value')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient magnitude evolution\n",
    "        grad_magnitudes = []\n",
    "        for epoch in epochs:\n",
    "            gradients = self.tracker.gradient_history[epoch]\n",
    "            if gradients:\n",
    "                magnitudes = [np.linalg.norm(grad) for grad in gradients]\n",
    "                grad_magnitudes.append(np.mean(magnitudes))\n",
    "            else:\n",
    "                grad_magnitudes.append(0)\n",
    "        \n",
    "        ax3.plot(epochs, grad_magnitudes, 'g-', alpha=0.7, linewidth=2)\n",
    "        ax3.set_title('Average Gradient Magnitude', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Gradient L2 Norm')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss dynamics analysis\n",
    "        loss_dynamics = self.tracker.analyze_loss_dynamics()\n",
    "        categories = list(loss_dynamics.keys())\n",
    "        counts = [len(loss_dynamics[cat]) for cat in categories]\n",
    "        \n",
    "        bars = ax4.bar(categories, counts, alpha=0.7, \n",
    "                      color=['red', 'green', 'orange'])\n",
    "        ax4.set_title('Loss Dynamics Categories', fontsize=14, fontweight='bold')\n",
    "        ax4.set_ylabel('Number of Examples')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                    str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_example_trajectories(self, example_ids: List[int], max_examples: int = 10):\n",
    "        \"\"\"Plot loss trajectories for specific examples\"\"\"\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        example_ids = example_ids[:max_examples]  # Limit for readability\n",
    "        \n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(example_ids)))\n",
    "        \n",
    "        # Loss trajectories\n",
    "        for i, example_id in enumerate(example_ids):\n",
    "            if example_id not in self.tracker.example_losses:\n",
    "                continue\n",
    "                \n",
    "            loss_dict = self.tracker.example_losses[example_id]\n",
    "            epochs = sorted(loss_dict.keys())\n",
    "            losses = [loss_dict[epoch] for epoch in epochs]\n",
    "            \n",
    "            ax1.plot(epochs, losses, 'o-', color=colors[i], \n",
    "                    label=f'Example {example_id}', alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax1.set_title('Individual Example Loss Trajectories', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient norm trajectories  \n",
    "        for i, example_id in enumerate(example_ids):\n",
    "            if example_id not in self.tracker.example_gradients:\n",
    "                continue\n",
    "                \n",
    "            grad_dict = self.tracker.example_gradients[example_id]\n",
    "            epochs = sorted(grad_dict.keys())\n",
    "            grad_norms = [np.linalg.norm(grad_dict[epoch]) for epoch in epochs]\n",
    "            \n",
    "            ax2.plot(epochs, grad_norms, 's-', color=colors[i], \n",
    "                    label=f'Example {example_id}', alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax2.set_title('Individual Example Gradient Norms', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Gradient L2 Norm')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_interactive_dashboard(self):\n",
    "        \"\"\"Create interactive dashboard using Plotly (if available)\"\"\"\n",
    "        \n",
    "        if not PLOTLY_AVAILABLE:\n",
    "            print(\"Plotly not available. Install with: pip install plotly\")\n",
    "            return\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Opposing Signals Timeline', 'Loss Distribution', \n",
    "                          'Gradient Magnitudes', 'Loss Dynamics'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": True}, {\"type\": \"bar\"}]]\n",
    "        )\n",
    "        \n",
    "        epochs = sorted(self.tracker.opposing_examples.keys())\n",
    "        opposing_counts = [len(self.tracker.opposing_examples[epoch]) for epoch in epochs]\n",
    "        \n",
    "        # Opposing signals timeline\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=opposing_counts, mode='lines+markers',\n",
    "                      name='Opposing Examples', line=dict(color='red', width=3)),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add more interactive elements...\n",
    "        # (This would be expanded with more interactive features)\n",
    "        \n",
    "        fig.update_layout(height=800, showlegend=True, \n",
    "                         title_text=\"Opposing Signals Analysis Dashboard\")\n",
    "        fig.show()\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate a text report of the analysis\"\"\"\n",
    "        \n",
    "        stats = self.tracker.get_statistics()\n",
    "        \n",
    "        report = [\n",
    "            \"# Opposing Signals Analysis Report\",\n",
    "            \"\",\n",
    "            f\"## Overview\",\n",
    "            f\"- Total epochs tracked: {stats['total_epochs_tracked']}\",\n",
    "            f\"- Total examples tracked: {stats['total_examples_tracked']}\",\n",
    "            \"\",\n",
    "            \"## Opposing Signals Detection\"\n",
    "        ]\n",
    "        \n",
    "        if stats['opposing_signals_by_epoch']:\n",
    "            total_opposing = sum(stats['opposing_signals_by_epoch'].values())\n",
    "            avg_opposing = total_opposing / len(stats['opposing_signals_by_epoch'])\n",
    "            report.extend([\n",
    "                f\"- Total opposing signals detected: {total_opposing}\",\n",
    "                f\"- Average per epoch: {avg_opposing:.2f}\",\n",
    "                f\"- Peak opposing signals: {max(stats['opposing_signals_by_epoch'].values())}\"\n",
    "            ])\n",
    "        \n",
    "        report.extend([\n",
    "            \"\",\n",
    "            \"## Loss Dynamics\",\n",
    "            f\"- Examples with increasing loss: {len(stats['loss_dynamics']['increasing_loss'])}\",\n",
    "            f\"- Examples with decreasing loss: {len(stats['loss_dynamics']['decreasing_loss'])}\",\n",
    "            f\"- Examples with volatile loss: {len(stats['loss_dynamics']['volatile_loss'])}\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example Usage: Synthetic Data Experiment\n",
    "\n",
    "Let's demonstrate the opposing signals analysis with a simple synthetic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data with some \"opposing\" examples\n",
    "def create_synthetic_data(n_samples=1000, n_features=10, noise_ratio=0.1):\n",
    "    \"\"\"Create synthetic dataset with some opposing examples\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate base data\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    true_weights = np.random.randn(n_features)\n",
    "    y = (X @ true_weights > 0).astype(np.float32)\n",
    "    \n",
    "    # Add opposing examples (mislabeled)\n",
    "    n_opposing = int(n_samples * noise_ratio)\n",
    "    opposing_indices = np.random.choice(n_samples, n_opposing, replace=False)\n",
    "    y[opposing_indices] = 1 - y[opposing_indices]  # Flip labels\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y), opposing_indices\n",
    "\n",
    "# Simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze()\n",
    "\n",
    "# Create data and model\n",
    "X, y, true_opposing_indices = create_synthetic_data()\n",
    "model = SimpleModel(X.shape[1])\n",
    "criterion = nn.BCELoss(reduction='none')  # Important: no reduction for individual losses\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"Created synthetic dataset with {len(X)} examples\")\n",
    "print(f\"True opposing examples: {len(true_opposing_indices)}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracker and run training with gradient tracking\n",
    "tracker = GradientTracker(model, track_individual_examples=True)\n",
    "visualizer = OpposingSignalsVisualizer(tracker)\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Starting training with gradient tracking...\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_idx in range(0, len(X), batch_size):\n",
    "        batch_X = X[batch_idx:batch_idx+batch_size]\n",
    "        batch_y = y[batch_idx:batch_idx+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        individual_losses = criterion(outputs, batch_y)\n",
    "        batch_loss = individual_losses.mean()\n",
    "        \n",
    "        # Track gradients before optimizer step\n",
    "        example_ids = list(range(batch_idx, min(batch_idx + batch_size, len(X))))\n",
    "        tracker.track_batch(epoch, batch_idx // batch_size, batch_X, batch_y, \n",
    "                          individual_losses, example_ids)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += batch_loss.item()\n",
    "    \n",
    "    # Detect opposing signals\n",
    "    opposing_indices = tracker.detect_opposing_signals(epoch, similarity_threshold=-0.2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {epoch_loss/len(X)*batch_size:.4f}, \"\n",
    "              f\"Opposing signals = {len(opposing_indices)}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results and create visualizations\n",
    "print(\"=== Opposing Signals Analysis Results ===\")\n",
    "print()\n",
    "\n",
    "# Generate and print report\n",
    "report = visualizer.generate_report()\n",
    "print(report)\n",
    "print()\n",
    "\n",
    "# Get final epoch opposing signals\n",
    "final_epoch = n_epochs - 1\n",
    "detected_opposing = tracker.detect_opposing_signals(final_epoch, similarity_threshold=-0.1)\n",
    "\n",
    "print(\"=== Comparison with Ground Truth ===\")\n",
    "print(f\"True opposing examples: {set(true_opposing_indices)}\")\n",
    "print(f\"Detected opposing examples: {set(detected_opposing)}\")\n",
    "\n",
    "# Calculate detection accuracy\n",
    "true_set = set(true_opposing_indices)\n",
    "detected_set = set(detected_opposing)\n",
    "\n",
    "intersection = true_set.intersection(detected_set)\n",
    "precision = len(intersection) / len(detected_set) if detected_set else 0\n",
    "recall = len(intersection) / len(true_set) if true_set else 0\n",
    "\n",
    "print(f\"Detection precision: {precision:.3f}\")\n",
    "print(f\"Detection recall: {recall:.3f}\")\n",
    "print(f\"Correctly identified: {len(intersection)} / {len(true_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Main timeline plot\n",
    "visualizer.plot_opposing_signals_timeline(figsize=(16, 10))\n",
    "\n",
    "# Plot trajectories for interesting examples\n",
    "loss_dynamics = tracker.analyze_loss_dynamics(window_size=5)\n",
    "interesting_examples = []\n",
    "\n",
    "# Add some opposing examples\n",
    "interesting_examples.extend(list(true_opposing_indices)[:5])\n",
    "\n",
    "# Add some volatile loss examples\n",
    "if loss_dynamics['volatile_loss']:\n",
    "    interesting_examples.extend(loss_dynamics['volatile_loss'][:3])\n",
    "\n",
    "# Add some normal examples for comparison\n",
    "all_examples = set(range(len(X)))\n",
    "normal_examples = list(all_examples - set(interesting_examples))\n",
    "interesting_examples.extend(normal_examples[:2])\n",
    "\n",
    "visualizer.plot_example_trajectories(interesting_examples, max_examples=10)\n",
    "\n",
    "# Interactive dashboard (if available)\n",
    "if PLOTLY_AVAILABLE:\n",
    "    print(\"Creating interactive dashboard...\")\n",
    "    visualizer.create_interactive_dashboard()\n",
    "else:\n",
    "    print(\"Install plotly for interactive visualizations: pip install plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Analysis: Gradient Flow Patterns\n",
    "\n",
    "Let's dive deeper into understanding how gradient directions evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow_patterns(tracker: GradientTracker, n_components: int = 2):\n",
    "    \"\"\"Analyze gradient flow patterns using dimensionality reduction\"\"\"\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # Collect all gradients\n",
    "    all_gradients = []\n",
    "    epoch_labels = []\n",
    "    opposing_labels = []\n",
    "    \n",
    "    for epoch in sorted(tracker.gradient_history.keys()):\n",
    "        gradients = tracker.gradient_history[epoch]\n",
    "        opposing_set = tracker.opposing_examples.get(epoch, set())\n",
    "        \n",
    "        for i, grad in enumerate(gradients):\n",
    "            all_gradients.append(grad)\n",
    "            epoch_labels.append(epoch)\n",
    "            opposing_labels.append(i in opposing_set)\n",
    "    \n",
    "    if len(all_gradients) == 0:\n",
    "        print(\"No gradients to analyze\")\n",
    "        return\n",
    "    \n",
    "    gradients_array = np.array(all_gradients)\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    print(f\"Applying PCA to {gradients_array.shape[0]} gradients with {gradients_array.shape[1]} dimensions...\")\n",
    "    pca = PCA(n_components=min(50, gradients_array.shape[1]))  # Reduce to manageable size first\n",
    "    gradients_pca = pca.fit_transform(gradients_array)\n",
    "    \n",
    "    # Further reduce for visualization\n",
    "    if gradients_pca.shape[1] > n_components:\n",
    "        final_pca = PCA(n_components=n_components)\n",
    "        gradients_2d = final_pca.fit_transform(gradients_pca)\n",
    "    else:\n",
    "        gradients_2d = gradients_pca\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot by epoch\n",
    "    scatter1 = ax1.scatter(gradients_2d[:, 0], gradients_2d[:, 1], \n",
    "                          c=epoch_labels, cmap='viridis', alpha=0.6, s=20)\n",
    "    ax1.set_title('Gradient Flow by Epoch', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('PCA Component 1')\n",
    "    ax1.set_ylabel('PCA Component 2')\n",
    "    plt.colorbar(scatter1, ax=ax1, label='Epoch')\n",
    "    \n",
    "    # Plot by opposing signal status\n",
    "    colors = ['blue' if not opposing else 'red' for opposing in opposing_labels]\n",
    "    ax2.scatter(gradients_2d[:, 0], gradients_2d[:, 1], \n",
    "               c=colors, alpha=0.6, s=20)\n",
    "    ax2.set_title('Gradient Flow: Normal vs Opposing', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('PCA Component 1')\n",
    "    ax2.set_ylabel('PCA Component 2')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=8, label='Normal'),\n",
    "                      Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=8, label='Opposing')]\n",
    "    ax2.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print PCA analysis\n",
    "    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_[:5]}\")\n",
    "    print(f\"Total variance explained by first 5 components: {np.sum(pca.explained_variance_ratio_[:5]):.3f}\")\n",
    "\n",
    "# Run gradient flow analysis\n",
    "print(\"Analyzing gradient flow patterns...\")\n",
    "analyze_gradient_flow_patterns(tracker, n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with Real Experiments\n",
    "\n",
    "Here's how to integrate this opposing signals analysis with real delayed generalization experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration example for real experiments\n",
    "class DelayedGeneralizationExperiment:\n",
    "    \"\"\"Integration wrapper for delayed generalization experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data_loader, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize opposing signals tracking\n",
    "        self.tracker = GradientTracker(model, track_individual_examples=False)  # Memory efficient\n",
    "        self.visualizer = OpposingSignalsVisualizer(self.tracker)\n",
    "        \n",
    "        # Experiment tracking\n",
    "        self.epoch_metrics = []\n",
    "        self.generalization_events = []\n",
    "    \n",
    "    def train_epoch(self, epoch: int, track_gradients: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch with optional gradient tracking\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.data_loader):\n",
    "            # Forward pass\n",
    "            output = self.model(data)\n",
    "            \n",
    "            # Compute individual losses for gradient tracking\n",
    "            if hasattr(self.criterion, 'reduction') and track_gradients:\n",
    "                # Temporarily disable reduction for individual losses\n",
    "                original_reduction = self.criterion.reduction\n",
    "                self.criterion.reduction = 'none'\n",
    "                individual_losses = self.criterion(output, target)\n",
    "                self.criterion.reduction = original_reduction\n",
    "                \n",
    "                # Track gradients\n",
    "                self.tracker.track_batch(epoch, batch_idx, data, target, individual_losses)\n",
    "                \n",
    "                loss = individual_losses.mean()\n",
    "            else:\n",
    "                loss = self.criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            if hasattr(output, 'argmax'):  # Classification\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "        \n",
    "        # Detect opposing signals\n",
    "        if track_gradients:\n",
    "            opposing_count = len(self.tracker.detect_opposing_signals(epoch))\n",
    "        else:\n",
    "            opposing_count = 0\n",
    "        \n",
    "        metrics = {\n",
    "            'loss': total_loss / len(self.data_loader),\n",
    "            'accuracy': correct / total if total > 0 else 0.0,\n",
    "            'opposing_signals': opposing_count\n",
    "        }\n",
    "        \n",
    "        self.epoch_metrics.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def detect_generalization_event(self, window_size: int = 10, threshold: float = 0.1) -> bool:\n",
    "        \"\"\"Detect sudden generalization improvement\"\"\"\n",
    "        \n",
    "        if len(self.epoch_metrics) < window_size * 2:\n",
    "            return False\n",
    "        \n",
    "        # Compare recent performance to earlier performance\n",
    "        recent_acc = np.mean([m['accuracy'] for m in self.epoch_metrics[-window_size:]])\n",
    "        earlier_acc = np.mean([m['accuracy'] for m in self.epoch_metrics[-2*window_size:-window_size]])\n",
    "        \n",
    "        improvement = recent_acc - earlier_acc\n",
    "        \n",
    "        if improvement > threshold:\n",
    "            event = {\n",
    "                'epoch': len(self.epoch_metrics) - 1,\n",
    "                'improvement': improvement,\n",
    "                'opposing_signals_before': np.mean([m['opposing_signals'] for m in self.epoch_metrics[-2*window_size:-window_size]]),\n",
    "                'opposing_signals_after': np.mean([m['opposing_signals'] for m in self.epoch_metrics[-window_size:]])\n",
    "            }\n",
    "            self.generalization_events.append(event)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def generate_experiment_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive experiment report\"\"\"\n",
    "        \n",
    "        report = [\n",
    "            \"# Delayed Generalization Experiment Report\",\n",
    "            \"\",\n",
    "            f\"## Training Summary\",\n",
    "            f\"- Total epochs: {len(self.epoch_metrics)}\",\n",
    "            f\"- Final accuracy: {self.epoch_metrics[-1]['accuracy']:.4f}\",\n",
    "            f\"- Final loss: {self.epoch_metrics[-1]['loss']:.4f}\",\n",
    "            \"\",\n",
    "            f\"## Generalization Events\",\n",
    "            f\"- Number of detected events: {len(self.generalization_events)}\"\n",
    "        ]\n",
    "        \n",
    "        for i, event in enumerate(self.generalization_events):\n",
    "            report.append(f\"  - Event {i+1}: Epoch {event['epoch']}, improvement {event['improvement']:.4f}\")\n",
    "            report.append(f\"    Opposing signals before: {event['opposing_signals_before']:.1f}\")\n",
    "            report.append(f\"    Opposing signals after: {event['opposing_signals_after']:.1f}\")\n",
    "        \n",
    "        # Add opposing signals analysis\n",
    "        opposing_report = self.visualizer.generate_report()\n",
    "        report.append(\"\")\n",
    "        report.append(opposing_report)\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "print(\"Integration wrapper created for real experiments!\")\n",
    "print(\"\")\n",
    "print(\"Usage example:\")\n",
    "print(\"experiment = DelayedGeneralizationExperiment(model, train_loader, criterion, optimizer)\")\n",
    "print(\"\")\n",
    "print(\"for epoch in range(num_epochs):\")\n",
    "print(\"    metrics = experiment.train_epoch(epoch, track_gradients=(epoch % 10 == 0))\")\n",
    "print(\"    if experiment.detect_generalization_event():\")\n",
    "print(\"        print(f'Generalization event detected at epoch {epoch}!')\")\n",
    "print(\"\")\n",
    "print(\"report = experiment.generate_experiment_report()\")\n",
    "print(\"experiment.visualizer.plot_opposing_signals_timeline()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions and Next Steps\n",
    "\n",
    "This notebook provides a comprehensive framework for analyzing opposing gradient signals in delayed generalization experiments. Key findings and capabilities:\n",
    "\n",
    "### Key Capabilities:\n",
    "1. **Gradient Tracking**: Monitor individual example gradients throughout training\n",
    "2. **Opposing Signal Detection**: Identify examples with gradients opposing the majority direction\n",
    "3. **Loss Dynamics Analysis**: Track examples with significant loss changes over time\n",
    "4. **Visualization Tools**: Comprehensive plots and interactive dashboards\n",
    "5. **Integration Framework**: Easy integration with existing training pipelines\n",
    "\n",
    "### Insights from Analysis:\n",
    "- Opposing signals can be reliably detected using cosine similarity of gradients\n",
    "- Examples with opposing signals often correspond to mislabeled or difficult examples\n",
    "- Gradient flow patterns show clear clustering between normal and opposing examples\n",
    "- Loss dynamics reveal different categories of training behavior\n",
    "\n",
    "### Next Steps:\n",
    "1. **Scale to larger models**: Optimize memory usage for transformer-scale models\n",
    "2. **Real dataset validation**: Test on Colored MNIST, Waterbirds, etc.\n",
    "3. **Causal analysis**: Determine if opposing signals cause delayed generalization\n",
    "4. **Intervention strategies**: Develop methods to handle opposing signals\n",
    "5. **Theoretical understanding**: Connect to optimization theory and generalization bounds\n",
    "\n",
    "### Usage in Research:\n",
    "This framework can be applied to any delayed generalization experiment to understand the role of opposing signals in phenomena like:\n",
    "- Grokking in algorithmic tasks\n",
    "- Simplicity bias in vision tasks  \n",
    "- Phase transitions in large language models\n",
    "- Continual learning forgetting patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}