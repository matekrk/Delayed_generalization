# Algorithmic Datasets for Delayed Generalization

## ðŸ“‹ Overview

Algorithmic datasets are particularly valuable for studying delayed generalization because they offer:
- **Controllable complexity**: Parameter choices affect difficulty
- **Clear ground truth**: Mathematical correctness is unambiguous  
- **Reproducible phenomena**: Consistent patterns across runs
- **Interpretable results**: Can analyze learned algorithms

## ðŸ§® Modular Arithmetic

### Dataset Description
Tasks involving arithmetic operations modulo a prime number p.

**Standard Operations**:
- Addition: `(a + b) mod p`
- Subtraction: `(a - b) mod p` 
- Multiplication: `(a * b) mod p`
- Division: `(a / b) mod p` (where b has multiplicative inverse)

### Key Parameters
```python
prime = 97  # or 113, common choices
operation = "addition"  # "+", "-", "*", "/"
train_fraction = 0.5  # Use 50% of all possible equations
vocab_size = prime + 3  # numbers + operation + equals + padding
```

### Data Format
```
Input sequence:  [a, +, b, =]
Target sequence: [a, +, b, =, result]

Example: [23, +, 45, =] â†’ [23, +, 45, =, 68]
```

### Grokking Timeline
- **p=97**: Typically 1000-5000 epochs for addition
- **p=113**: Often 2000-8000 epochs  
- **Multiplication**: Usually slower than addition
- **Division**: Most complex, longest timeline

### Implementation Example
```python
def generate_modular_arithmetic(prime, operation, train_fraction):
    """Generate modular arithmetic dataset"""
    all_pairs = [(a, b) for a in range(prime) for b in range(prime)]
    
    if operation == "addition":
        results = [(a + b) % prime for a, b in all_pairs]
    elif operation == "multiplication":  
        results = [(a * b) % prime for a, b in all_pairs]
    # ... other operations
    
    # Create train/test split
    n_train = int(len(all_pairs) * train_fraction)
    train_indices = random.sample(range(len(all_pairs)), n_train)
    
    return train_data, test_data
```

## ðŸ”„ Permutation Groups

### Dataset Description
Tasks involving composition of permutation group elements.

**Group Types**:
- **S_n**: Symmetric groups (all permutations of n elements)
- **A_n**: Alternating groups (even permutations)
- **Cyclic Groups**: Generated by single cycle
- **Dihedral Groups**: Symmetries of regular polygons

### Key Parameters
```python
group_size = 5  # S_5 has 120 elements
composition_table = generate_group_table(group_size)
train_fraction = 0.5
```

### Data Format
```
Input: [perm1, *, perm2, =]
Output: [perm1, *, perm2, =, perm1_compose_perm2]
```

### Grokking Characteristics
- **Longer Timeline**: Often 5000+ epochs
- **Group Size Effect**: Larger groups = longer grokking
- **Composition Complexity**: Non-abelian groups harder
- **Representation Matters**: Cycle notation vs matrix affects learning

## ðŸ“ˆ Sequence Operations

### Copy Task
```
Input:  [1, 2, 3, 4, <COPY>]
Output: [1, 2, 3, 4, <COPY>, 1, 2, 3, 4]
```

### Reverse Task  
```
Input:  [1, 2, 3, 4, <REV>]
Output: [1, 2, 3, 4, <REV>, 4, 3, 2, 1]
```

### Sort Task
```
Input:  [3, 1, 4, 2, <SORT>]
Output: [3, 1, 4, 2, <SORT>, 1, 2, 3, 4]
```

### Delayed Generalization Patterns
- **Length Generalization**: Train on short sequences, test on long
- **Compositional**: Combine multiple operations
- **Algorithmic**: Learning sorting algorithms vs memorization

## ðŸŽ² Graph Problems

### Shortest Path
- **Input**: Graph adjacency matrix + source/target nodes
- **Output**: Shortest path length or path sequence
- **Grokking**: Transition from memorization to Dijkstra-like reasoning

### Connectivity  
- **Input**: Graph edges
- **Output**: Connected components or reachability
- **Pattern**: Learning graph traversal algorithms

### Graph Coloring
- **Input**: Graph structure
- **Output**: Valid coloring with minimum colors
- **Delayed Learning**: Constraint satisfaction vs memorization

## âš™ï¸ Experimental Setup Guidelines

### Model Architecture
```python
# Standard transformer for algorithmic tasks
model_config = {
    "n_layers": 2,
    "n_heads": 4, 
    "d_model": 128,
    "d_ff": 512,
    "max_seq_len": 10,  # Depends on task
    "vocab_size": task_vocab_size
}
```

### Training Configuration
```python
training_config = {
    "learning_rate": 1e-3,
    "weight_decay": 1e-2,  # Critical for grokking!
    "batch_size": 512,
    "max_epochs": 10000,  # Be patient
    "eval_frequency": 100,
    "patience": float('inf')  # Don't early stop
}
```

### Monitoring Setup
```python
metrics_to_track = [
    "train_accuracy",
    "test_accuracy", 
    "train_loss",
    "test_loss",
    "epoch_to_generalization",  # When test acc > threshold
    "weight_norms",
    "attention_patterns"  # For interpretability
]
```

## ðŸŽ¯ Reproduction Tips

### For Modular Arithmetic
1. **Start Simple**: p=97, addition, standard hyperparameters
2. **Weight Decay**: Don't skip this - try 1e-2 to start
3. **Long Training**: Plan for 10,000+ epochs
4. **Multiple Seeds**: Grokking timing varies significantly
5. **Monitor Closely**: Use logging tools like wandb

### For Permutation Groups
1. **Begin Small**: S_4 or S_5 before larger groups
2. **Representation**: Try different permutation encodings
3. **Patience**: Even longer timelines than modular arithmetic
4. **Curriculum**: Consider starting with abelian groups

### Common Pitfalls
- **Insufficient Weight Decay**: Most common cause of no grokking
- **Early Stopping**: Patience is key
- **Wrong Splits**: Ensure no data leakage
- **Architecture**: Too small/large can prevent grokking

## ðŸ“Š Benchmark Results

### Modular Addition (p=97)
- **Baseline Transformer**: Grokking at ~3000 epochs (50% seeds)
- **With Optimal Hyperparams**: Grokking at ~1500 epochs (90% seeds)
- **No Weight Decay**: Rarely groks within 10,000 epochs

### S_5 Permutation Composition
- **Standard Setup**: Grokking at ~8000 epochs (30% seeds) 
- **Optimized**: Grokking at ~5000 epochs (70% seeds)
- **Timeline Variance**: 3x-5x variation common

## ðŸ”— Data Generation Scripts

See the individual subdirectories for:
- `generate_modular.py`: Modular arithmetic data generation
- `generate_groups.py`: Permutation group datasets  
- `generate_sequences.py`: Sequence operation tasks
- `generate_graphs.py`: Graph problem datasets

## ðŸ“š References

1. Power et al. (2022). "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
2. Liu et al. (2022). "Towards Understanding Grokking: An Effective Theory of Representation Learning"  
3. Nanda et al. (2023). "Progress measures for grokking via mechanistic interpretability"
4. Varma et al. (2023). "Explaining grokking through circuit efficiency"